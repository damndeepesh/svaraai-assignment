{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SvaraAI Reply Classification Pipeline\n",
    "\n",
    "A clean, end-to-end notebook to:\n",
    "- Load and preprocess the dataset\n",
    "- Train and compare baseline models (TF-IDF + classical ML)\n",
    "- Fine-tune DistilBERT\n",
    "- Evaluate with Accuracy and F1, then compare and conclude\n",
    "\n",
    "Dataset file expected at `reply_classification_dataset.csv` in the same directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# If running in a fresh environment, uncomment the following to install deps\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# %pip install -q numpy pandas scikit-learn matplotlib seaborn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# %pip install -q transformers datasets evaluate torch accelerate\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# %pip install -q lightgbm\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# If running in a fresh environment, uncomment the following to install deps\n",
    "# %pip install -q numpy pandas scikit-learn matplotlib seaborn\n",
    "# %pip install -q transformers datasets evaluate torch accelerate\n",
    "# %pip install -q lightgbm\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification, TrainingArguments,Trainer,DataCollatorWithPadding\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = \"reply_classification_dataset.csv\"\n",
    "\n",
    "# Ensure dataset exists\n",
    "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}. Place the CSV next to this notebook.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "RAW = pd.read_csv(DATA_PATH)\n",
    "print(RAW.head())\n",
    "print(\"Rows:\", len(RAW))\n",
    "\n",
    "# Normalize column names\n",
    "RAW.columns = [c.strip().lower() for c in RAW.columns]\n",
    "assert set([\"reply\", \"label\"]).issubset(RAW.columns), RAW.columns\n",
    "\n",
    "# Label normalization map (handle casing/typos)\n",
    "label_map = {\n",
    "    \"positive\": \"positive\",\n",
    "    \"pos\": \"positive\",\n",
    "    \"negative\": \"negative\",\n",
    "    \"neg\": \"negative\",\n",
    "    \"neutral\": \"neutral\",\n",
    "}\n",
    "\n",
    "# Clean label strings\n",
    "RAW[\"label\"] = (\n",
    "    RAW[\"label\"].astype(str).str.strip().str.lower()\n",
    ")\n",
    "\n",
    "# Try to coerce known misspellings/casing to canonical labels\n",
    "RAW[\"label\"] = RAW[\"label\"].replace({\n",
    "    \"positive\": \"positive\",\n",
    "    \"pos\": \"positive\",\n",
    "    \"neg\": \"negative\",\n",
    "    \"negative\": \"negative\",\n",
    "    \"neutral\": \"neutral\",\n",
    "})\n",
    "\n",
    "# Filter unknown labels\n",
    "VALID_LABELS = {\"positive\", \"negative\", \"neutral\"}\n",
    "RAW = RAW[RAW[\"label\"].isin(VALID_LABELS)].copy()\n",
    "\n",
    "# Text cleaning function\n",
    "CLEAN_RE = re.compile(r\"[^a-z0-9\\s']+\")\n",
    "MULTISPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text.strip().lower()\n",
    "    t = CLEAN_RE.sub(\" \", t)\n",
    "    t = MULTISPACE.sub(\" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "RAW[\"reply\"] = RAW[\"reply\"].astype(str).apply(clean_text)\n",
    "\n",
    "# Drop missing/empty rows\n",
    "RAW = RAW[(RAW[\"reply\"].str.len() > 0) & (RAW[\"label\"].str.len() > 0)].copy()\n",
    "\n",
    "RAW[\"label\"].value_counts(normalize=True).rename(\"proportion\").to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split\n",
    "\n",
    "X = RAW[\"reply\"].values\n",
    "y = RAW[\"label\"].values\n",
    "\n",
    "# train (70%), temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "# val (15%), test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "len(X_train), len(X_val), len(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline models: TF-IDF + classical ML\n",
    "\n",
    "# Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# Candidate models\n",
    "candidates = {\n",
    "    \"logreg\": LogisticRegression(max_iter=1000, class_weight=None, n_jobs=None),\n",
    "    \"linear_svc\": LinearSVC(),\n",
    "    \"sgd_log\": SGDClassifier(loss=\"log_loss\", max_iter=1000),\n",
    "    \"mnb\": MultinomialNB(),\n",
    "    \"cnb\": ComplementNB(),\n",
    "    \"lgbm\": LGBMClassifier(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=3,\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_name = None\n",
    "best_f1 = -1.0\n",
    "best_model = None\n",
    "\n",
    "for name, estimator in candidates.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", vectorizer),\n",
    "        (\"clf\", estimator),\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    results.append({\"model\": name, \"val_accuracy\": acc, \"val_f1_macro\": f1})\n",
    "    if (f1 > best_f1) or (np.isclose(f1, best_f1) and acc > [r[\"val_accuracy\"] for r in results if r[\"model\"]==best_name][0] if best_name else False):\n",
    "        best_f1 = f1\n",
    "        best_name = name\n",
    "        best_model = pipe\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"val_f1_macro\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best baseline on test set\n",
    "\n",
    "# Refit best model on train+val for final testing\n",
    "best_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(\"Best baseline:\", best_name)\n",
    "print(classification_report(y_test, y_pred_test, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test, labels=[\"negative\", \"neutral\", \"positive\"])\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"negative\", \"neutral\", \"positive\"],\n",
    "            yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "plt.title(f\"Baseline ({best_name}) Confusion Matrix\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune DistilBERT\n",
    "\n",
    "label_list = [\"negative\", \"neutral\", \"positive\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"text\": list(X_train),\n",
    "    \"label\": [label_to_id[l] for l in y_train],\n",
    "})\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"text\": list(X_val),\n",
    "    \"label\": [label_to_id[l] for l in y_val],\n",
    "})\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"text\": list(X_test),\n",
    "    \"label\": [label_to_id[l] for l in y_test],\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
    "\n",
    "train_tok = train_ds.map(tokenize_function, batched=True)\n",
    "val_tok = val_ds.map(tokenize_function, batched=True)\n",
    "test_tok = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "cols_to_remove = [c for c in [\"text\"] if c in train_tok.column_names]\n",
    "train_tok = train_tok.remove_columns(cols_to_remove)\n",
    "val_tok = val_tok.remove_columns(cols_to_remove)\n",
    "test_tok = test_tok.remove_columns(cols_to_remove)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Backward-compatible TrainingArguments (supports older transformers without evaluation_strategy)\n",
    "try:\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"bert_out\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "except TypeError:\n",
    "    try:\n",
    "        args = TrainingArguments(\n",
    "            output_dir=\"bert_out\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=50,\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Very old versions may require per_gpu_* arguments\n",
    "        args = TrainingArguments(\n",
    "            output_dir=\"bert_out\",\n",
    "            learning_rate=2e-5,\n",
    "            per_gpu_train_batch_size=16,\n",
    "            per_gpu_eval_batch_size=32,\n",
    "            num_train_epochs=3,\n",
    "            logging_steps=50,\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "val_metrics = trainer.evaluate()\n",
    "print(val_metrics)\n",
    "\n",
    "test_metrics = trainer.evaluate(test_tok)\n",
    "print(test_metrics)\n",
    "\n",
    "bert_val_acc = val_metrics.get(\"eval_accuracy\")\n",
    "bert_val_f1 = val_metrics.get(\"eval_f1\")\n",
    "bert_test_acc = test_metrics.get(\"eval_accuracy\")\n",
    "bert_test_f1 = test_metrics.get(\"eval_f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and conclude\n",
    "\n",
    "# Capture baseline metrics (from validation/test)\n",
    "baseline_val_pred = best_model.predict(X_val)\n",
    "baseline_val_acc = accuracy_score(y_val, baseline_val_pred)\n",
    "baseline_val_f1 = f1_score(y_val, baseline_val_pred, average=\"macro\")\n",
    "\n",
    "baseline_test_acc = accuracy_score(y_test, y_pred_test)\n",
    "baseline_test_f1 = f1_score(y_test, y_pred_test, average=\"macro\")\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\"model\": f\"baseline_{best_name}\", \"split\": \"val\", \"accuracy\": baseline_val_acc, \"f1\": baseline_val_f1},\n",
    "    {\"model\": f\"baseline_{best_name}\", \"split\": \"test\", \"accuracy\": baseline_test_acc, \"f1\": baseline_test_f1},\n",
    "    {\"model\": \"distilbert\", \"split\": \"val\", \"accuracy\": bert_val_acc, \"f1\": bert_val_f1},\n",
    "    {\"model\": \"distilbert\", \"split\": \"test\", \"accuracy\": bert_test_acc, \"f1\": bert_test_f1},\n",
    "])\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- If DistilBERT outperforms the best baseline on F1 (especially macro-F1), prefer it in production for better generalization on nuanced language.\n",
    "- If the dataset is small and latency/cost constraints are strict, the TF-IDF + Linear model is simpler, faster, and may be sufficient.\n",
    "- Consider model monitoring and periodic re-training as data drifts.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
